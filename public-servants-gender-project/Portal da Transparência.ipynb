{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFRN - EEC2006 - TOPICOS ESPECIAIS F\n",
    "## Terceiro trabalho \n",
    "\n",
    "Componentes:\n",
    "* **20171021275 - Fabio Fonseca de Oliveira**\n",
    "* **2016102462  - Júlio César Melo Gomes de Oliveira**\n",
    "* **20171021201 - Tiago Fernandes de Miranda**\n",
    "\n",
    "Notebook com solução do terceiro trabalho proposto na disciplina. A seguir breve descrição de cada solução bem como suas informações específicas.\n",
    "\n",
    "O presente notebook possui a primeira parte da solução, apresentando o procedimento utilizado para o Web Scrapping, usado para o nome dos servidores da UFRN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Portal da Transparência\n",
    "==\n",
    "\n",
    "[Portal da Transparência](http://www.portaldatransparencia.gov.br/) is a Brazilian government portal dedicated to making public all expenditures of the federal government. It has a list of all expenses and money transfers the federal government has made.\n",
    "\n",
    "1.1 Motivations\n",
    "\n",
    "- How many employees do the IES (*instituições de ensino superior*) have?\n",
    "- What is the gender gap between the employees? \n",
    "    - https://www.dicionariodenomesproprios.com.br/\n",
    "    - https://gender-api.com/\n",
    "    - https://pypi.python.org/pypi/Genderize\n",
    "    - http://fmeireles.com/blog/rstats/genderbr-predizer-sexo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Number of employees by IES\n",
    "==\n",
    "\n",
    "- [Units of Ministry of Education]( http://www.portaltransparencia.gov.br/servidores/OrgaoExercicio-ListaOrgaos.asp?CodOS=15000)\n",
    "\n",
    "### 2.1 Identifying the URL structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "from requests import get\n",
    "\n",
    "# specify the url\n",
    "url = 'http://www.portaldatransparencia.gov.br/servidores/\\\n",
    "OrgaoExercicio-ListaOrgaos.asp?CodOS=15000'\n",
    "\n",
    "# packages the request, send the request and catch the response\n",
    "response = get(url)\n",
    "\n",
    "# extract the text\n",
    "text = response.text\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Understanding the HTML structure of a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "type(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# information about IES is within a table element\n",
    "unit_table = html_soup.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there are two table elements \n",
    "len(unit_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the second one is the target\n",
    "unit_rows  = unit_table[1].find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(unit_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the first tr is the header of table\n",
    "unit_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_rows = unit_rows[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_rows[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Page\n",
    "ttag_p = html_soup.find('p', class_ = 'paginaAtual').text\n",
    "text1 = ttag_p.split(' ', 1 )\n",
    "text2 = text1[1].split('/', 1 )\n",
    "lastPage = text2[1]\n",
    "print(lastPage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extracting the RH code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_rh_code = unit_rows[0].find('td', class_ = 'firstChild').text\n",
    "unit_rh_code\n",
    "for k in unit_rows:\n",
    "    p = k.find('td', class_ = 'firstChild').text\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extracting the name of IES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_name = unit_rows[0].find('a').text\n",
    "unit_name\n",
    "for k in unit_rows:\n",
    "    p = k.find('a').text\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Extracting the number of employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_number_of_employees = unit_rows[0].find('td', attrs = {'style':'text-align: right;'}).text\n",
    "unit_number_of_employees = int(unit_number_of_employees)\n",
    "unit_number_of_employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 The script for a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists to store the scraped data in\n",
    "rh_codes = []\n",
    "names = []\n",
    "number_of_employees = []\n",
    "\n",
    "# Extract data from individual ies rows\n",
    "for row in unit_rows:\n",
    "    \n",
    "    # rh codes\n",
    "    codes = row.find('td', class_ = 'firstChild').text\n",
    "    rh_codes.append(codes)\n",
    "    \n",
    "    # ies names\n",
    "    name = row.find('a').text\n",
    "    names.append(name)\n",
    "    \n",
    "    # number of employees\n",
    "    employees = row.find('td', attrs = {'style':'text-align: right;'}).text\n",
    "    number_of_employees.append(int(employees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let’s check the data collected so far. \n",
    "# Pandas makes it easy for us to see whether \n",
    "# we’ve scraped our data successfully.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "web_scraping_df = pd.DataFrame({'Code': rh_codes,\n",
    "                       'IES_name': names,\n",
    "                       'Number_employees': number_of_employees})\n",
    "print(web_scraping_df.info())\n",
    "web_scraping_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. The script for multiple pages\n",
    "\n",
    "Scraping multiple pages is a bit more challenging. We’ll build upon our one-page script by doing three more things:\n",
    "\n",
    "- Making all the requests we want from within the loop.\n",
    "- Controlling the loop’s rate to avoid bombarding the server with requests.\n",
    "- Monitoring the loop while it runs.\n",
    "\n",
    "We’ll scrape all pages (8 pages) that contains information about the number of employees of IES. Each page has 15 lines (excluding the header) of target information, so we’ll scrape data for 120 IES. But not all pages have 15 lines, the last one is incomplete. \n",
    "\n",
    "\n",
    "### 2.6.1 Changing the URL’s parameters\n",
    "\n",
    "As shown earlier, the URLs follow a certain logic as the web pages change.\n",
    "\n",
    "http://www.portaltransparencia.gov.br/servidores/OrgaoExercicio-ListaOrgaos.asp?CodOS=15000&Pagina=5\n",
    "\n",
    "As we are making the requests, we’ll only have to vary the values of only the last parameter of the URL: the <span style=\"background-color: #F9EBEA; color:##C0392B\">Pagina</span> parameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pages = [str(i) for i in range(1,9)]\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2  Piecing everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists to store the scraped data in\n",
    "rh_codes = []\n",
    "names = []\n",
    "number_of_employees = []\n",
    "\n",
    "# Preparing the monitoring of the loop\n",
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "# For each page\n",
    "for page in pages:\n",
    "    \n",
    "    #url \n",
    "    url = 'http://www.portaltransparencia.gov.br/servidores/\\\n",
    "    OrgaoExercicio-ListaOrgaos.asp?CodOS=15000&Pagina={}'.format(page).replace(\" \", \"\")\n",
    "        \n",
    "    # Make a get request\n",
    "    response = get(url)\n",
    "        \n",
    "    # Pause the loop\n",
    "    sleep(randint(5,10))\n",
    "    \n",
    "    # Monitor the requests\n",
    "    requests += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "              \n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "              \n",
    "    # Break the loop if the number of requests is greater than expected\n",
    "    if requests > 72:\n",
    "        warn('Number of requests was greater than expected.')  \n",
    "        break \n",
    "        \n",
    "    # information about IES is within a table element\n",
    "    unit_table = html_soup.find_all('table')\n",
    "    \n",
    "    # the second one is the target\n",
    "    unit_rows  = unit_table[1].find_all('tr')\n",
    "    unit_rows = unit_rows[1:]\n",
    "    \n",
    "    # Extract data from individual ies rows\n",
    "    for row in unit_rows:\n",
    "    \n",
    "        # rh codes\n",
    "        codes = row.find('td', class_ = 'firstChild').text\n",
    "        rh_codes.append(codes)\n",
    "    \n",
    "        # ies names\n",
    "        name = row.find('a').text\n",
    "        names.append(name)\n",
    "    \n",
    "        # number of employees\n",
    "        employees = row.find('td', attrs = {'style':'text-align: right;'}).text\n",
    "        number_of_employees.append(int(employees))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let’s check the data collected so far. \n",
    "# Pandas makes it easy for us to see whether \n",
    "# we’ve scraped our data successfully.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "web_scraping_df = pd.DataFrame({'Code': rh_codes,\n",
    "                       'IES_name': names,\n",
    "                       'Number_employees': number_of_employees})\n",
    "print(web_scraping_df.info())\n",
    "web_scraping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web_scraping_df.to_csv('number_of_employees.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Plotting and analyzing the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#to switch to seaborn defaults, simply call the set() function.\n",
    "sns.set()\n",
    "\n",
    "# The four preset contexts, in order of relative size, are paper, notebook, talk, and poster\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# plot a univariate distribution of observations.\n",
    "sns.distplot(web_scraping_df[\"Number_employees\"],bins=50, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(web_scraping_df[\"Number_employees\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Mean: %d' % (web_scraping_df[\"Number_employees\"].mean()))\n",
    "print('Median: %d' % (web_scraping_df[\"Number_employees\"].median()))\n",
    "print('Standard deviation: %d' % (web_scraping_df[\"Number_employees\"].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.  Adição dos servidores da UFRN num dataframe para tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verificar o número de páginas que deverão ser checadas para adição dos servidores na lista\n",
    "\n",
    "import timeit\n",
    "\n",
    "df = pd.read_csv('number_of_employees.csv',encoding = 'utf-8')\n",
    "column_names = df.columns\n",
    "code = df['Code']\n",
    "pages = []\n",
    "i = 1\n",
    "for c in code:\n",
    "        url ='http://www.portaltransparencia.gov.br/servidores/\\\n",
    "        OrgaoExercicio-ListaServidores.asp?CodOrg={}'.format(c).replace(\" \", \"\")\n",
    "\n",
    "        # Make a get request\n",
    "        response = get(url)\n",
    "\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        type(html_soup)\n",
    "\n",
    "        # Pause the loop\n",
    "        sleep(randint(5,10))\n",
    "\n",
    "        # Monitor the requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        # Throw a warning for non-200 status codes\n",
    "        if response.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "        # Break the loop if the number of requests is greater than expected\n",
    "        if requests > 372:\n",
    "            warn('Number of requests was greater than expected.')  \n",
    "            break \n",
    "\n",
    "        ttag_p = html_soup.find('p', class_ = 'paginaAtual').text\n",
    "        text1 = ttag_p.split(' ', 1 )\n",
    "        text2 = text1[1].split('/', 1 )\n",
    "        lastPage = text2[1]\n",
    "        pages.append(lastPage)\n",
    "        print(lastPage)\n",
    "        #time.sleep(time.localtime(time.time())[15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pesquisa no portal dos servidores com a UFRN\n",
    "#Aquisição de páginas e adição de cada servidor numa Lista\n",
    "\n",
    "code = '26243'\n",
    "url ='http://www.portaltransparencia.gov.br/servidores/\\\n",
    "OrgaoExercicio-ListaServidores.asp?CodOrg={}'.format(code).replace(\" \", \"\")\n",
    "\n",
    "# Make a get request\n",
    "response = get(url)\n",
    "\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "type(html_soup)\n",
    "\n",
    "ttag_p = html_soup.find('p', class_ = 'paginaAtual').text\n",
    "text1 = ttag_p.split(' ', 1 )\n",
    "text2 = text1[1].split('/', 1 )\n",
    "lastPage = text2[1]\n",
    "\n",
    "pages = [str(i) for i in range(2,int(lastPage)+1)]\n",
    "pages\n",
    "\n",
    "unit_table = html_soup.find_all('table')\n",
    "unit_rows  = unit_table[1].find_all('tr')\n",
    "\n",
    "func = []\n",
    "for k in unit_rows:\n",
    "    p = k.find('a').text\n",
    "    func.append(p)\n",
    "    \n",
    "# Preparing the monitoring of the loop\n",
    "start_time = time()\n",
    "requests = 0    \n",
    "\n",
    "for p in pages:\n",
    "        url ='http://www.portaltransparencia.gov.br/servidores/\\\n",
    "        OrgaoExercicio-ListaServidores.asp?CodOrg={}&Pagina={}'.format(code,p).replace(\" \", \"\")\n",
    "\n",
    "        # Make a get request\n",
    "        response = get(url)\n",
    "\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        type(html_soup)\n",
    "\n",
    "        # Pause the loop\n",
    "        sleep(randint(5,10))\n",
    "\n",
    "        # Monitor the requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        # Throw a warning for non-200 status codes\n",
    "        if response.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "        unit_table = html_soup.find_all('table')\n",
    "        unit_rows  = unit_table[1].find_all('tr')\n",
    "\n",
    "        for k in unit_rows:\n",
    "            pson = k.find('a').text\n",
    "            func.append(pson)\n",
    "\n",
    "func = [p for p in func if p !='Nome do servidor']\n",
    "len(func)\n",
    "pages = [str(i) for i in range(2,int('9')+1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remoção dos espaços adicionais no Final dos nomes completos\n",
    "people = []\n",
    "for f in func:\n",
    "    people.append(f.rstrip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code = '26243' # Cod. de pesquisa da UFRN\n",
    "gen = 'I' # Indefinido\n",
    "codes_func = []\n",
    "gen_func = []\n",
    "for p in people:\n",
    "    codes_func.append(code)\n",
    "    gen_func.append(gen)\n",
    "func_scraping_df = pd.DataFrame({'Inst_code': codes_func,\n",
    "                       'Serv_name': people,\n",
    "                       'Serv_gen': gen_func})\n",
    "print(func_scraping_df.info())\n",
    "func_scraping_df    \n",
    "func_scraping_df.to_csv('func_scraping_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from genderize import Genderize\n",
    "df = pd.read_csv('func_scraping_df.csv',encoding = 'utf-8')\n",
    "column_names = df.columns\n",
    "func_name = df['Serv_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Organição dos dados para armazenamento\n",
    "\n",
    "first_n = []\n",
    "prob_n = []\n",
    "gender_n = []\n",
    "count = 0\n",
    "for name in func_name:\n",
    "    gender = Genderize().get([name])\n",
    "    gender_n.append(gender[0])\n",
    "    first_n.append(gender[0]['gender'])\n",
    "    sleep(randint(6,12))\n",
    "    count += 1\n",
    "    print(count,\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Organição dos dados a partir do primeiro nome para tratamento\n",
    "\n",
    "df_n = pd.read_csv('func_scraping_df.csv',encoding = 'utf-8')\n",
    "column_names = df_n.columns\n",
    "\n",
    "for i,gen  in df_n.iterrows():\n",
    "    df_n.loc[i,'Serv_gen'] = first_n[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_n.to_csv('func_scraping_df_n.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
